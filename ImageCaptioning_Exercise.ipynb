{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ImageCaptioning_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEqqed3ndMqJ6K8ucshils",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ninigapa0/SAMSUNG_ImageCaptioning/blob/main/ImageCaptioning_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvfuNONaN1as"
      },
      "source": [
        "Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLJIygzQLJNF"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNN_NvDhBhVM"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JAbmAmI1RRY"
      },
      "source": [
        "%cd\n",
        "%cd /content/gdrive/MyDrive\n",
        "import os\n",
        "if not (os.path.isdir(\"SAMSUNG_ImageCaptioning\")):\n",
        "    os.mkdir(\"SAMSUNG_ImageCaptioning\")\n",
        "% cd SAMSUNG_ImageCaptioning\n",
        "\n",
        "\n",
        "\n",
        "#Download pretrained weights, test sets\n",
        "!gdown --id 1d5K2tSHPyLFyNiR-F__zxCIXqAMJDCk2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svsC8m897tgO"
      },
      "source": [
        "!unzip -o SAMSUNG_ImageCaptioning.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIavgXcfauqz"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwnfNGVoagq0"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "import argparse\n",
        "from skimage import io\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import warnings\n",
        "import requests\n",
        "from models import Encoder as Encoder_answer\n",
        "from models import Attention as Attention_answer\n",
        "from models import DecoderWithAttention as Decoder_answer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4mQBs0HjFiw"
      },
      "source": [
        "# Set GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "\n",
        "# Load word map (word2ix)\n",
        "word_map='WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'\n",
        "with open(word_map, 'r') as j:\n",
        "    word_map = json.load(j)\n",
        "rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFZwiMVT6Pfv"
      },
      "source": [
        "#Fix seed\n",
        "random_seed=0\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed_all(random_seed) # if use multi-GPU\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(random_seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ve3WFmMvfOmX"
      },
      "source": [
        "1.Implement Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaM_ZPRaE0GQ"
      },
      "source": [
        "Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hmL2YTH9QL2"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "\n",
        "        ##################################### Fill in blank lines ############################################\n",
        "\n",
        "        # Fill in the empty line to complete following 'TODO'\n",
        "        # TODO: complete forwarding path of Encoder network and calculate final 'output' feature\n",
        "        # 1. calculate 'out' using 'images' $ 'self.resnet'\n",
        "        # 2. calculate 'out' using 'out' & 'self.adaptive_pool'\n",
        "        # 3. modify shape of 'out' by using tensor.permute() function\n",
        "\n",
        "\n",
        "        out = # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xn_Jp8rSgTg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v0A7Ld8fSVn"
      },
      "source": [
        "Attention\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oL5TXEz3CJIT"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "        ##################################### Fill in blank lines ############################################\n",
        "\n",
        "        # Fill in the empty line to complete following 'TODO'\n",
        "        # TODO:calculate attention_weighted_encoding (context vector) and alpha (weight of each pixel)\n",
        "        # 1. calculate 'embedding1' using 'self.encoder_att' $ 'encoder_output'\n",
        "        # 2. calculate 'embedding2' using 'self.decoder_att' $ 'decoder_hidden'\n",
        "        # 3. calculate 'alpha' using self.softmax & att\n",
        "        # 4. calculate 'attention_weighted_encoding' using encoder_out & alpha\n",
        "        # hint for 4: you can use tensor.unsqueeze(dim) to add new dimension\n",
        "\n",
        "\n",
        "        embedding1 =   # (batch_size, num_pixels, attention_dim)\n",
        "        embedding2 =   # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(embedding1 + embedding2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha =        # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding =  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30JxSw0OfX2g"
      },
      "source": [
        "Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zd9caLV3CLHb"
      },
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, encoded_captions, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xcph11U-fdhH"
      },
      "source": [
        "# Model parameters\n",
        "embed_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "\n",
        "import copy\n",
        "\n",
        "\n",
        "encoder=Encoder().to(device)\n",
        "encoder.eval()\n",
        "decoder=DecoderWithAttention(attention_dim=attention_dim, embed_dim=embed_dim, decoder_dim=decoder_dim, vocab_size=len(word_map)).to(device)\n",
        "decoder.eval()\n",
        "\n",
        "\n",
        "encoder_answer=Encoder_answer().to(device)\n",
        "encoder_answer.eval()\n",
        "\n",
        "decoder_answer=Decoder_answer(attention_dim=attention_dim, embed_dim=embed_dim, decoder_dim=decoder_dim, vocab_size=len(word_map)).to(device)\n",
        "decoder_answer.eval()\n",
        "\n",
        "def make_parameters_same(net,answer):\n",
        "  state_dict=net.state_dict()\n",
        "  for name,weights in answer.named_parameters():\n",
        "    state_dict[name]=weights\n",
        "  net.load_state_dict(state_dict)\n",
        "  return net\n",
        "\n",
        "encoder=make_parameters_same(encoder,encoder_answer)\n",
        "decoder=make_parameters_same(decoder,decoder_answer)\n",
        "\n",
        "\n",
        "#Check Encoder\n",
        "img=torch.randn(5,3,256,256).to(device)\n",
        "encoder_out1=encoder(img)\n",
        "encoder_out2=encoder_answer(img)\n",
        "if (abs(encoder_out1-encoder_out2).sum()==0):\n",
        "  print(\"Valid Encoder!!\")\n",
        "  #Check Decoder\n",
        "  captions=torch.randint(0,10,(5,13),dtype=int).to(device)\n",
        "  caption_lengths=torch.randint(0,10,(5,1),dtype=int).to(device)\n",
        "  a0,a1,a2,a3,a4=decoder(encoder_out1.detach().clone(),copy.deepcopy(captions),copy.deepcopy(caption_lengths))\n",
        "  b0,b1,b2,b3,b4=decoder_answer(encoder_out1,captions,caption_lengths)\n",
        "  if ((abs(a0-b0).sum()+abs(a1-b1).sum()+abs(a3-b3).sum()+abs(a4-b4).sum())==0):\n",
        "    print(\"Valid Decoder!!\")\n",
        "  else:\n",
        "    print(\"Need to fix Decoder\")\n",
        "\n",
        "\n",
        "else:\n",
        "  print(\"Need to fix Encoder\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jNk9y79vWGj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u79_Uy8yhVC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktC_n0IFvTWP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vut1gy3mE3gI"
      },
      "source": [
        "2.Implement beam search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF9QFXv0COJB"
      },
      "source": [
        "def caption_image_beam_search(encoder, decoder, url, word_map, beam_size=3):\n",
        "    \"\"\"\n",
        "    Reads an image and captions it with beam search.\n",
        "\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param url: url of  image\n",
        "    :param word_map: word map\n",
        "    :param beam_size: number of sequences to consider at each decode-step\n",
        "    :return: caption, weights for visualization\n",
        "    \"\"\"\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(word_map)\n",
        "\n",
        "    # Read image and process\n",
        "\n",
        "    image_nparray = np.asarray(bytearray(requests.get(url).content), dtype=np.uint8)\n",
        "    img = cv2.imdecode(image_nparray, cv2.IMREAD_COLOR)\n",
        "\n",
        "    #img = io.imread(image_path)\n",
        "    if len(img.shape) == 2:\n",
        "        img = img[:, :, np.newaxis]\n",
        "        img = np.concatenate([img, img, img], axis=2)\n",
        "    img = skimage.transform.resize(img, (256, 256))\n",
        "    img = img.transpose(2, 0, 1)\n",
        "    #img = img / 255.\n",
        "    img = torch.FloatTensor(img).to(device)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([normalize])\n",
        "    image = transform(img)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    ############################# Fill in blank lines ###########################################################\n",
        "    image =   # (1, 3, 256, 256). Use .unsqueeze(K) function to add new dimension into 'k' th dim\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size  # (s)\n",
        "        prev_word_inds = prev_word_inds.type(torch.LongTensor).to(device)\n",
        "        next_word_inds = next_word_inds.type(torch.LongTensor).to(device)\n",
        "\n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != word_map['<end>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    return seq, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIH0AqZfPWAg"
      },
      "source": [
        "def visualize_att(image, seq, alphas, rev_word_map, smooth=True):\n",
        "    \"\"\"\n",
        "    Visualizes caption with weights at every word.\n",
        "\n",
        "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
        "\n",
        "    :param image_path: path to image that has been captioned\n",
        "    :param seq: caption\n",
        "    :param alphas: weights\n",
        "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
        "    :param smooth: smooth weights?\n",
        "    \"\"\"\n",
        "    #image = Image.open(image_path)\n",
        "    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n",
        "\n",
        "    words = [rev_word_map[ind] for ind in seq]\n",
        "\n",
        "    for t in range(len(words)):\n",
        "        if t > 50:\n",
        "            break\n",
        "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
        "\n",
        "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
        "        plt.imshow(image)\n",
        "        current_alpha = alphas[t, :]\n",
        "        if smooth:\n",
        "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n",
        "        else:\n",
        "            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n",
        "        if t == 0:\n",
        "            plt.imshow(alpha, alpha=0)\n",
        "        else:\n",
        "            plt.imshow(alpha, alpha=0.8)\n",
        "        plt.set_cmap(cm.Greys_r)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y7XAHsFbZCs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZMJUNJbZEz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnaEdf_4bZju"
      },
      "source": [
        "Load pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh10E4VBbZju"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "warnings.filterwarnings(action='ignore') \n",
        "MODEL_PATH='BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL_PATH, map_location=str(device))\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "\n",
        "warnings.filterwarnings(action='default') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7QsvrC1xSwV"
      },
      "source": [
        "def caption_image( url,beam_size,encoder, decoder, word_map, rev_word_map):\n",
        "  img = Image.open(requests.get(url, stream=True).raw)\n",
        "\n",
        "  # Encode, decode with attention and beam search\n",
        "  seq, alphas = caption_image_beam_search(encoder, decoder, url, word_map, beam_size)\n",
        "  alphas = torch.FloatTensor(alphas)\n",
        "\n",
        "  # Visualize caption and attention of best sequence\n",
        "  visualize_att(img, seq, alphas, rev_word_map,True)\n",
        "  warnings.filterwarnings(action='default') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrwjVdcuE9ag"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLN_pA0E-Ap"
      },
      "source": [
        "Check the image captioning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s2kqZ42FLwA"
      },
      "source": [
        "Get image url from internet( ex) https://cocodataset.org/#explore)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWLRuktlz-Cj"
      },
      "source": [
        "caption_image(url=\"https://img4.yna.co.kr/photo/reuters/2021/07/22/PRU20210722044001055_P4.jpg\",beam_size=25,encoder=encoder,decoder=decoder,word_map=word_map, rev_word_map=rev_word_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGHALBnFOekl"
      },
      "source": [
        "url=input()\n",
        "caption_image(url=url,beam_size=25,encoder=encoder,decoder=decoder,word_map=word_map, rev_word_map=rev_word_map)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydipXmcSOemd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}